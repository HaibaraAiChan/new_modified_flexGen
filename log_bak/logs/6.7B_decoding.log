<run_flexgen>: args.model: facebook/opt-6.7b
model size: 12.386 GB, cache size: 0.562 GB, hidden size (prefill): 0.009 GB
init weight...



the useful data start from here -------------------------------------
benchmark - generate
args.gen_len  32
input  torch.Size([4, 256])
============ generate (only decode) ============
generate start -----
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention prefill--------
mha prefill----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

self attention decode =======
mha_gen decode----------------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_write_buf  None

------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_write_buf  None

generate stop *******
Outputs:
----------------------------------------------------------------------
0: Paris is the capital city of France and the most visited city in the world. It is the most visited city in the world, with more than 30 million visitors each year. Paris is the
----------------------------------------------------------------------
3: Paris is the capital city of France and the most visited city in the world. It is the most visited city in the world, with more than 30 million visitors each year. Paris is the
----------------------------------------------------------------------

TorchDevice: cuda:0
  cur_mem: 12.7859 GB,  peak_mem: 13.4502 GB
TorchDevice: cpu
  cur_mem: 0.0000 GB,  peak_mem: 0.0000 GB
model size: 12.386 GB	cache size: 0.562 GB	hidden size (p): 0.009 GB
peak gpu mem: 13.450 GB	projected: False
prefill latency: 0.475 s	prefill throughput: 2155.127 token/s
decode latency: 1.012 s	decode throughput: 122.545 token/s
total latency: 1.487 s	total throughput: 86.078 token/s
