<run_flexgen>: args.model: facebook/opt-6.7b
model size: 12.386 GB, cache size: 0.562 GB, hidden size (generate): 0.009 GB
init weight...

the useful data start from here -------------------------------------
benchmark - generate
args.gen_len  32
input  torch.Size([4, 256])
============ generate ============
generate start -----
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([256, 128, 128]), torch.Size([256, 128, 128])

decoding stop
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 256]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 257]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 258]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 259]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 260]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 261]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 262]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 263]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 264]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 265]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 266]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 267]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 268]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 269]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 270]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 271]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 272]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 273]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 274]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 275]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 276]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 277]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 278]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 279]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 280]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 281]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 282]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 283]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 284]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 285]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 286]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
decoding start
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf torch.Size([1, 128, 128]), torch.Size([1, 128, 128])

decoding stop
decoding start
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
decoding start
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
cache_read_buf  None
weight_read_buf  None
attention_mask  TorchTensor(shape=torch.Size([4, 287]), dtype=torch.bool, device=cuda:0)
cache_write_buf  None

decoding stop
generate stop *******
Outputs:
----------------------------------------------------------------------
0: Paris is the capital city of France and the most visited city in the world. It is the most visited city in the world, with more than 30 million visitors each year. Paris is the
----------------------------------------------------------------------
3: Paris is the capital city of France and the most visited city in the world. It is the most visited city in the world, with more than 30 million visitors each year. Paris is the
----------------------------------------------------------------------

TorchDevice: cuda:0
  cur_mem: 12.7859 GB,  peak_mem: 13.4502 GB
TorchDevice: cpu
  cur_mem: 0.0000 GB,  peak_mem: 0.0000 GB
model size: 12.386 GB	cache size: 0.562 GB	hidden size (p): 0.009 GB
peak gpu mem: 13.450 GB	projected: False
generate latency: 0.191 s	generate throughput: 5356.600 token/s
decode latency: 1.020 s	decode throughput: 121.568 token/s
total latency: 1.211 s	total throughput: 105.683 token/s

   model structure 
InputEmbed
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
OutputEmbed